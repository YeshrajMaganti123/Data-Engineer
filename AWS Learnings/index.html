<!DOCTYPE html>
<html>
<head>
    <link rel="stylesheet" type="text/css" href="styles.css">
</head>
<body style="font-size:20px;">
<p><a href="#C1">WELCOME</a></p>
<p><a href="#C2">AWS Glue</a></p>
<p><a href="#C3">AWS EMR</a></p>
<p><a href="#C4">Amazon S3</a></p> 
<p><a href="#C5">Amazon Redshift</a></p>
<p><a href="#C6">Amazon Kinesis</a></p>
<p><a href="#C7">Amazon DynamoDB</a></p>
<p><a href="#C8">GIT</a></p>
<p><a href="#C9">How would you compare adf to other ETL tools you worked with?</a></p>
<p><a href="#C10">Authoring Tool (Azure Data Factory UI/ADF Editor</a></p>
<p><a href="#C11">Spark session function </a></p>



<h2 id="C1">Welcome</h2>
<p style="text-align: center;"> Concepts for Data Engineer</p>

<div>
   <h2 id="C2">1. AWS Glue</h2>
<p>

1. AWS Glue simplifies data integration for analytics users with serverless capabilities.  <br>
2. AWS Glue helps you create reusable custom visual transforms to share the ETL logic.<br>
3. AWS Glue supports S3, RDS, Redshift, SQL, and DynamoDB, and offers built-in transformations. <br>
4. AWS Glue Fully managed ETL with serverless Apache Spark for your jobs. <br>
5. AWS Glue Serverless data integration, simplifying, speeding up, and reducing costs for data preparation. <br>

</p>

<h2 id="C3">2. AWS EMR</h2>
<p>  

1. Amazon EMR (Elastic MapReduce) simplifies big data processing with Hadoop and Spark on AWS.<br>
2. Using the YARN as the default can process the data to schedule the jobs and manage the cluster resources. <br>
3. Different kinds of processing needs are available for different frameworks. <br>
4. EMR streamlines data processing and analysis, automating resource management for cost-effectiveness.<br>
5. EMR enables insights from big data, real-time processing, and diverse use cases, from logs to machine learning.<br>

</p>

<h2 id="C4">3. Amazon S3</h2>
<p>

1. S3 is AWS's secure, scalable, and versatile data storage service. <br>
2. S3 is essential for reliable, cost-effective, and scalable data storage with limitless capacity and accessibility via web or API. <br>
3. S3's versatility, data protection, and AWS integration are crucial for scalable, resilient healthcare, finance, and media applications. <br>
4. S3's key role is securely storing vast data, essential for numerous cloud applications and services. <br>
5. S3 Low latency, seamless AWS integration, robust security with encryption and access control. <br>
6. S3 empowers data storage, backup, hosting, and analytics at any scale, essential for modern cloud computing. <br>

</p>

<h2 id="C5">4. Amazon Redshift</h2>
<p>

1. Consider it as a Data Warehouse and it is meant to bring the datasets from different places.<br>
2. It distributes the workloads(more access to querying for BI, DA and DS).<br>
3. It supports petabytes of data it is also controlled by adding nodes.<br>
4. Optimizing for large queries.<br>
5. Elastic Scaling.<br>
6. Managed- Almost zero maintenance.<br>
7. Optimized Query Performance.<br>
8. Support Thousands of users with a single cluster.<br>
9. Flexible Pricing Model<br>
10. Intergrated with other AWS services.<br>
</p>

<h2 id="C6">5. Amazon Kinesis</h2>
<p>

1. Amazon Kinesis is a suite of real-time data streaming and processing services provided by Amazon Web Services (AWS).<br>
2. It is immensely useful for organizations that need to handle and analyze streaming data at a scale.<br>
3. AWS Kinesis handles high-velocity data from IoT, web apps, and logs for ingestion, processing, and reaction.<br>
4. AWS Kinesis offers Data Streams, Firehose, and Analytics for building real-time solutions in analytics, machine learning, and monitoring.<br>
5. AWS Kinesis suite provides real-time insights, anomaly detection, and rapid event response, vital for e-commerce, gaming, and IoT industries.<br>

</p>

<h2 id="C7">6. Amazon DynamoDB</h2>
<p>

1. Amazon DynamoDB: AWS-managed NoSQL for fast, flexible, and scalable data storage.<br>
2. DynamoDB: Valuable for low-latency data access with serverless, auto-scaling, and fault-tolerant architecture.<br>
3. It is essential for responsive data storage in mobile, online, gaming, and IoT applications to have DynamoDB's adaptability, global tables, encryption, and access control.<br>

</p>

<h2 id="C8">7. Git</h2>
<p>
1. Git: Free, open-source, distributed version control for projects of all sizes, fast and efficient. <br>
2. Git offers the functionality, performance, security, and flexibility most teams and individual developers require.<br>
3. I managed GIT source code repositories, maintained local mirrors, and handled tasks like branching, tagging, merging, and maintenance for Windows and Mac builds. <br>
4. Coordinate and assist developers in establishing and applying suitable branching and labeling/naming conventions using GIT source control. <br>
5. Configured and administered GitHub enterprise, also, I handled migrations from SVN to GitHub.<br>
</p>

<h2 id="C9">8. How would you compare ADF to other ETL tools you worked with?</h2>
<p>

1. Azure Data Factory emphasizes Extract-and-Load (EL) and Transform-and-Load (TL) over traditional ETL. <br>
2. Choosing between ADF and other ETL tools depends on project requirements, as each tool has unique strengths and weaknesses for various contexts.<br>
3. ADF offers a robust ETL solution within the Microsoft Azure ecosystem. <br>
4. Compared to other ETL tools, ADF shines with its smooth Azure service integration and user-friendly graphical interface. <br>
5. ADF excels in orchestrating complex data workflows, hybrid data integration, and robust monitoring and debugging features. <br>
 
</p>

<h2 id="C10">9. Authoring Tool (Azure Data Factory UI/ADF Editor</h2>
<p>

1.	This allows data engineers and developers to design, configure, and monitor data pipelines visually. You can use the UI to create, edit, and publish data pipelines and activities. <br>

2. Data Pipelines:<br>
•	Representing a series of data-driven activities and transformations that move and process data from source to destination. <br>

2.	Linked Services:<br>
•	Linked services are connections to data sources or destinations, <br>
•	They define the connection details and credentials required to access these resources. <br>

3.	Triggers:<br>
•	Triggers are used to schedule and execute data pipelines. ADF supports various types of triggers, including time-based (e.g., recurring schedules), event-based (e.g., data arrival), and manual triggers.

<br>
Monitoring and Management:<br>

ADF provides monitoring and management capabilities through Azure Monitor, Azure Data Factory Monitor, and Azure Data Studio.<br>
These tools allow you to track pipeline execution, review logs, and troubleshoot issues.<br>
</p>

<h2 id="C11">10. Describe how you'd schedule jobs using Databricks notebooks.</h2>
<p>

1. Create or Open a Notebook<br>
2. Define Notebook Code<br>
3. Create a Job<br>
4. Specify Job Settings<br>
5. Schedule the Job<br>
6. Review and Create<br>
The job will run automatically based on the defined schedule or trigger, executing the notebook's code on the specified cluster.<br>

</p>

<h2 id="C12">11.How can you parameterize a Databricks notebook?</h2>
<p>


1.	In Databricks notebooks, you can parameterize a notebook by using Widgets. Widgets allow you to create input parameters that users can set when running the notebook.<br>

2. You can create input parameters that users can set when running the notebook, making it more flexible and interactive. <br>

</p>

<h2 id="C13">12. How does the Delta table handle schema evolution? Can you tell me the scenario and how you have done this?</h2>
<p>

1.	Delta Lake is an open-source storage layer that brings ACID transactions to Apache Spark and big data workloads.<br>
2.	Scenario: Adding a New Column to an Existing Delta Table <br>
3.	Suppose you have a Delta table named `sales_data` <br>
4.	You want to add a new column called `customer_id` to this table.<br>
•	Create a Delta Lake Table<br>
•	Add the New Column: To add the new column, you can use a SQL `ALTER TABLE` command. <br>
•	Update Data <br>
•	Backward Compatibility: Delta Lake is designed to be backward compatible. Existing queries that access the table will continue to work as expected, even if they don't use the new `customer_id` column. <br>
•	Schema Evolution Tracking: Delta Lake automatically tracks schema evolution history in the transaction log. You can view the schema history using the `DESCRIBE HISTORY` command. <br>
•	Data Integrity <br>
•	Optimization <br>
•	Testing and Validation: As with any schema change, it's crucial to thoroughly test and validate the changes in a development or staging environment before applying them to production data. <br>

5. Delta Lake simplifies schema evolution by providing built-in functionality to update schemas, ensuring backward compatibility, tracking schema history, and maintaining data integrity. This makes it a powerful choice for managing evolving data in big data and Spark environments.<br>
</p>

<h2 id="C14">13. Spark session function </h2>
<p>
1. SparkSession is the unified entry point to use all the features of Apache Spark, including Spark SQL, DataFrame API, and Dataset API
2. SparkSession is the entry point for working with structured data using Spark's SQL and DataFrame API. <br>
3. You can use SparkSession to create DataFrames, register them as temporary tables, and perform SQL queries on them. This makes it easy to work with structured data in Spark using SQL-like syntax.<br>
4. SparkSession provides methods for reading data from various sources like Parquet, JSON, CSV, and Hive tables, as well as writing data back to these formats or saving it to other storage systems.<br>
5. You can set various configuration options for your Spark application through the SparkSession.<br>
<p>

<h2 id="C15">14. Spark read CSV </h2>
<p>
1. By using the spark. read method as shown above, we can easily read CSV files into a data frame for further analysis.
</p>

<h2 id="C16">15. Apache Parquet</h2>
<p>
1. Apache Parquet is an open-source, column-oriented data file format designed for efficient data storage and retrieval.
<p>

<h2 id="C17">16. Df.write.parquet</h2>
<p>
1. Using write.parquet is a common way to save the results of your Spark data processing pipelines into an efficient and columnar storage format for further analysis or for sharing with other systems that can read Parquet files efficiently.
</p>

<h2 id="C17">16. what is disturbed computing and how it is used in data engineering?</h2>
<p>
1.	Distributed Computing: <br>
Distributed computing refers to the use of multiple computers or servers to work together on a common task or problem. <br>
This approach is often used to process and analyze large datasets that cannot be handled by a single machine. <br>
Distributed computing frameworks like Apache Hadoop, Apache Spark, and Apache Flink are commonly used in data engineering to distribute and parallelize data processing tasks.<br>

2.	Data Engineering: <br>
Data engineering involves the collection, transformation, and storage of data to make it accessible and usable for data analysis and reporting.<br>
 Data engineers often work with distributed computing frameworks to handle large volumes of data efficiently.<br>

</p>
<p>
What is the difference between a surrogate key and a Natural key?

A natural key is a key that is derived from the data itself, such as a customer ID, a product code, or a date. A surrogate key is a key that is generated artificially, such as a sequential number, a GUID, or a hash.

</p>
<p>
Concept of a fact table and dimension table 

A fact table holds the data to be analyzed, and a dimension table stores data about the ways in which the data in the fact table can be analyzed.
</p>
SCD
A Slowly Changing Dimension (SCD) is a dimension that stores and manages both current and historical data over time in a data warehouse.
<p>

Acid properties in SQL server
atomicity, consistency, isolation, and durability.
<p>

<p>

Drawback of using the surrogate key
As the key value has no relation to the data of the table, so third normal form is violated.
</p>

<p>
OLAP, which stands for Online Analytical Processing, is a category of data processing and querying that is crucial in the context of data warehousing. OLAP is designed for complex data analysis and reporting, allowing users to interactively explore and analyze multidimensional data stored in data warehouses.
Multidimensional Data Model
Data Aggregation
Fast Query Performance
Data Slicing and Dicing
Drill-Down and Drill-Up
Data Visualization
Business Intelligence (BI)
 
</p>

<p>

OLTP, which stands for Online Transaction Processing, is a category of database systems and applications designed for managing and processing a high volume of online transactions. OLTP systems are optimized for day-to-day operational tasks, such as inserting, updating, and retrieving individual records in real-time, and they play a crucial role in various industries, including e-commerce, finance, healthcare, and more. 
Transactional Operations
Concurrent Access
ACID Properties
Normalized Data
Indexes
Read and Write Operations
Data Integrity Constraints
Data Volume

</p>

<p>
Data Staging

Data staging is a crucial step in the process of preparing and transforming data for use in data warehousing, business intelligence, analytics, or other data-related tasks. Staging involves the extraction, cleaning, and temporary storage of raw data from source systems before it is loaded into a data warehouse or other target system. The primary purpose of data staging is to ensure data quality and consistency before it enters the production environment.
</p>

<p>
Data Cleansing


Data cleansing, also known as data cleaning or data scrubbing, is the process of identifying and correcting errors, inconsistencies, inaccuracies, and incomplete or irrelevant information in a dataset. The goal of data cleansing is to improve data quality, ensuring that data is accurate, reliable, and suitable for its intended purpose.
</p>

<p>
Data Mining

Data mining is the process of discovering hidden patterns, trends, associations, and knowledge from large volumes of data. It involves using various techniques, algorithms, and tools to analyze data and extract valuable insights. Data mining is widely used in various fields, including business, finance, healthcare, marketing, and science, to make informed decisions and predictions.
</p>

<p>

Materialized View
A materialized view, often referred to as a materialized query table (MQT), is a database object that stores the result of a query as a physical table. Unlike regular (virtual) views, which are just saved SQL queries that execute dynamically whenever they are referenced, materialized views store the actual data from the query result, which can be precomputed and updated periodically. Materialized views are primarily used for improving query performance, data aggregation, and providing real-time access to summary data in data warehousing and reporting environments.

</p>

</html>
